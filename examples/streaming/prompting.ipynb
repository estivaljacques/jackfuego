{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bittensor as bt\n",
    "import pydantic\n",
    "from starlette.types import Send\n",
    "from functools import partial\n",
    "from typing import Callable, Awaitable, List\n",
    "import asyncio\n",
    "\n",
    "bt.trace()\n",
    "\n",
    "\n",
    "# This is a subclass of StreamingSynapse for prompting network functionality\n",
    "class StreamPrompting(bt.StreamingSynapse):\n",
    "    \"\"\"\n",
    "    StreamPrompting is a subclass of StreamingSynapse that is specifically designed for prompting network functionality.\n",
    "    It overrides abstract methods from the parent class to provide concrete implementations for processing streaming responses,\n",
    "    deserializing the response, and extracting JSON data.\n",
    "\n",
    "    Attributes:\n",
    "        roles: List of roles associated with the prompt.\n",
    "        messages: List of messages to be processed.\n",
    "        completion: A string to store the completion result.\n",
    "    \"\"\"\n",
    "\n",
    "    roles: List[str] = pydantic.Field(..., allow_mutation=False)\n",
    "    messages: List[str] = pydantic.Field(..., allow_mutation=False)\n",
    "    completion: str = None\n",
    "\n",
    "    async def process_streaming_response(self, response):\n",
    "        \"\"\"\n",
    "        Processes the streaming response by iterating through the content and decoding tokens.\n",
    "        Concatenates the decoded tokens into the completion attribute.\n",
    "\n",
    "        Args:\n",
    "            response: The response object containing the content to be processed.\n",
    "        \"\"\"\n",
    "        if self.completion is None:\n",
    "            self.completion = \"\"\n",
    "        async for chunk in response.content.iter_any():\n",
    "            tokens = chunk.decode('utf-8').split('\\n')\n",
    "            for token in tokens:\n",
    "                if token:\n",
    "                    self.completion += token\n",
    "\n",
    "    def deserialize(self):\n",
    "        \"\"\"\n",
    "        Deserializes the response by returning the completion attribute.\n",
    "\n",
    "        Returns:\n",
    "            str: The completion result.\n",
    "        \"\"\"\n",
    "        return self.completion\n",
    "\n",
    "    def extract_response_json(self, response):\n",
    "        \"\"\"\n",
    "        Extracts JSON data from the response, including headers and specific information related to dendrite and axon.\n",
    "\n",
    "        Args:\n",
    "            response: The response object from which to extract JSON data.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing extracted JSON data.\n",
    "        \"\"\"\n",
    "        headers = {k.decode('utf-8'): v.decode('utf-8') for k, v in response.__dict__[\"_raw_headers\"]}\n",
    "\n",
    "        def extract_info(prefix):\n",
    "            return {key.split('_')[-1]: value for key, value in headers.items() if key.startswith(prefix)}\n",
    "\n",
    "        return {\n",
    "            \"name\": headers.get('name', ''),\n",
    "            \"timeout\": float(headers.get('timeout', 0)),\n",
    "            \"total_size\": int(headers.get('total_size', 0)),\n",
    "            \"header_size\": int(headers.get('header_size', 0)),\n",
    "            \"dendrite\": extract_info('bt_header_dendrite'),\n",
    "            \"axon\": extract_info('bt_header_axon'),\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"completion\": self.completion,\n",
    "        }\n",
    "\n",
    "# This should encapsulate all the logic for generating a streaming response\n",
    "def sforward(synapse: StreamPrompting) -> StreamPrompting:\n",
    "    \"\"\"\n",
    "    Encapsulates the logic for generating a streaming response. It defines the tokenizer, model, and prompt functions,\n",
    "    and creates a streaming response using the provided synapse.\n",
    "\n",
    "    Args:\n",
    "        synapse: A StreamPrompting instance containing the messages to be processed.\n",
    "\n",
    "    Returns:\n",
    "        StreamPrompting: The streaming response object.\n",
    "    \"\"\"\n",
    "    def tokenizer(text):\n",
    "        return (ord(char) for char in text)\n",
    "\n",
    "    def model(ids):\n",
    "        return (chr(id) for id in ids)\n",
    "\n",
    "    async def prompt(text: str, send: Send):\n",
    "        # Simulate model inference\n",
    "        input_ids = tokenizer(text)\n",
    "        for token in model(input_ids):\n",
    "            await send({\"type\": \"http.response.body\", \"body\": (token + '\\n').encode('utf-8'), \"more_body\": True})\n",
    "            bt.logging.trace(f\"Streamed token: {token}\")\n",
    "            # Sleep to show the streaming effect\n",
    "            await asyncio.sleep(0.5)\n",
    "\n",
    "    message = synapse.messages[0]\n",
    "    token_streamer = partial(prompt, message)\n",
    "    return synapse.create_streaming_response(token_streamer)\n",
    "\n",
    "def blacklist(synapse: StreamPrompting) -> bool:\n",
    "    \"\"\"\n",
    "    Determines whether the synapse should be blacklisted.\n",
    "\n",
    "    Args:\n",
    "        synapse: A StreamPrompting instance.\n",
    "\n",
    "    Returns:\n",
    "        bool: Always returns False, indicating that the synapse should not be blacklisted.\n",
    "    \"\"\"\n",
    "    return False\n",
    "\n",
    "def priority(synapse: StreamPrompting) -> float:\n",
    "    \"\"\"\n",
    "    Determines the priority of the synapse.\n",
    "\n",
    "    Args:\n",
    "        synapse: A StreamPrompting instance.\n",
    "\n",
    "    Returns:\n",
    "        float: Always returns 0.0, indicating the default priority.\n",
    "    \"\"\"\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Axon instance on port 8099.\n",
    "axon = bt.axon(port=8099)\n",
    "\n",
    "# Attach the forward, blacklist, and priority functions to the Axon.\n",
    "# forward_fn: The function to handle forwarding logic.\n",
    "# blacklist_fn: The function to determine if a request should be blacklisted.\n",
    "# priority_fn: The function to determine the priority of the request.\n",
    "axon.attach(\n",
    "    forward_fn=sforward,\n",
    "    blacklist_fn=blacklist,\n",
    "    priority_fn=priority\n",
    ")\n",
    "\n",
    "# Start the Axon to begin listening for requests.\n",
    "axon.start()\n",
    "\n",
    "# Create a Dendrite instance to handle client-side communication.\n",
    "d = bt.dendrite()\n",
    "\n",
    "# Send a request to the Axon using the Dendrite, passing in a StreamPrompting instance with roles and messages.\n",
    "# The response is awaited, as the Dendrite communicates asynchronously with the Axon.\n",
    "resp = await d(\n",
    "    [axon],\n",
    "    StreamPrompting(roles=[\"user\"], messages=[\"hello this is a test of streaming.\"])\n",
    ")\n",
    "\n",
    "# The response object contains the result of the streaming operation.\n",
    "resp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
